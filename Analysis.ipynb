{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HpSMshRPaM9I"
      },
      "outputs": [],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "from pyspark.sql.functions import udf, lower, split, when, col\n",
        "from pyspark.sql.types import ArrayType, StringType, MapType, FloatType\n",
        "from pyspark.sql.functions import concat_ws"
      ],
      "metadata": {
        "id": "voE8H5q6aYyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import StringIndexer, Tokenizer, HashingTF, IDF, VectorAssembler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Qo7hU_FqbVVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Twitter BTC Sentiment Analysis\").getOrCreate()"
      ],
      "metadata": {
        "id": "7cWk_toZaxs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "df_small = spark.read.csv(\"data/balanced_twitter_btc_small.csv\", header=True, inferSchema=True)\n",
        "df_big = spark.read.csv(\"data/balanced_twitter_btc_big.csv\", header=True, inferSchema=True)\n",
        "df_unbalanced = spark.read.csv(\"data/unbalanced_twitter_btc_big.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "M1mdaRIGbbHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing\n",
        "def preprocess_text(df):\n",
        "    # Remove URLs\n",
        "    df = df.withColumn('cleaned_text', regexp_replace('text', '(https?://\\S+|www\\.\\S+)', ''))\n",
        "    \n",
        "    # Remove mentions and hashtags\n",
        "    df = df.withColumn('cleaned_text', regexp_replace('cleaned_text', '@\\w+|#\\w+', ''))\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    df = df.withColumn('cleaned_text', lower('cleaned_text'))\n",
        "    \n",
        "    # Remove special characters and numbers\n",
        "    df = df.withColumn('cleaned_text', regexp_replace('cleaned_text', '[^a-zA-Z\\s]', ''))\n",
        "    \n",
        "    return df\n",
        "\n",
        "df_small = preprocess_text(df_small)\n",
        "df_big = preprocess_text(df_big)\n",
        "df_unbalanced = preprocess_text(df_unbalanced)"
      ],
      "metadata": {
        "id": "preprocessingCell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction pipeline\n",
        "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=1000)\n",
        "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])"
      ],
      "metadata": {
        "id": "featureExtractionCell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train test split\n",
        "train_data, test_data = df_small.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Fit and transform the pipeline\n",
        "pipeline_model = pipeline.fit(train_data)\n",
        "train_features = pipeline_model.transform(train_data)\n",
        "test_features = pipeline_model.transform(test_data)"
      ],
      "metadata": {
        "id": "trainTestSplitCell"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "lr_model = lr.fit(train_features)\n",
        "\n",
        "# Make predictions\n",
        "predictions = lr_model.transform(test_features)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "modelTrainingCell"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}